{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5867d637-a233-4fa7-b9cc-cd80fc6a7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import hydrobm\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from hydrobm.calculate import calc_bm\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9adee2d-63d3-492d-bea0-aed6017499ec",
   "metadata": {},
   "source": [
    "### Description\n",
    "_________________\n",
    "This script requires data for precipitation, temperature, observed streamflow, simulated streamflow, subbasin order (for calculating upstream precipitation), and subbasin IDs. This script generates the timeseries for each HydroBM benchmark by iterating through a list of IDs. It then computes the skill score between the model simulated timeseries and benchmark timeseries for each benchmark.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec66c4f-a392-4d9c-a083-4d96ee6be103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file paths\n",
    "\n",
    "pobs= '../SMM_Models/hype/model/v10_model/final_model/hds_v3/Pobs.txt' # precipitation for each subbasin\n",
    "\n",
    "tobs= '../SMM_Models/hype/model/v10_model/final_model/hds_v3/Tobs.txt' # temperature for each subbasin\n",
    "\n",
    "qobs= '../SMM_Models/hype/model/v10_model/final_model/hds_v3/Qobs.txt' # observed flow for each analyzed subbasin\n",
    "\n",
    "#cout = '../SMM_Models/hype/model/v10_model/final_model/hds_v3/results/timeCOUT_DD.txt' # simulated flow at each subbasin\n",
    "\n",
    "cout = 'inputs/raven_streamflow.csv'\n",
    "\n",
    "geodata= '../SMM_Models/hype/model/v10_model/final_model/hds_v3/GeoData.txt' # subbasin downstream order\n",
    "\n",
    "gauge_info= 'inputs/gauge_info.csv' # list of subbasins to iterate through\n",
    "\n",
    "output_dir= './' # output directory\n",
    "\n",
    "# Define skill score calculation\n",
    "skill_score_metric = 'nse' # method of skill score, options are rmse and nse\n",
    "\n",
    "start_date = \"1980-10-01\"\n",
    "end_date = \"2015-09-30\"\n",
    "\n",
    "\n",
    "# Define calibration and validation periods\n",
    "calibration_ranges = [('1980-10-01', '1984-09-30'),\n",
    "               ('1989-10-01', '1998-09-30'),\n",
    "               ('2003-10-01', '2007-09-30'),\n",
    "               ('2012-10-01', '2015-09-30')]\n",
    "\n",
    "validation_ranges = [('1984-10-01', '1989-09-30'),\n",
    "               ('1998-10-01', '2003-09-30'),\n",
    "               ('2007-10-01', '2012-09-30')]\n",
    "\n",
    "\n",
    "# Specify the benchmarks and metrics to calculate\n",
    "benchmarks = [\n",
    "        # Streamflow benchmarks\n",
    "        \"mean_flow\",\n",
    "        \"median_flow\",\n",
    "        \"annual_mean_flow\",\n",
    "        \"annual_median_flow\",\n",
    "        \"monthly_mean_flow\",\n",
    "        \"monthly_median_flow\",\n",
    "        \"daily_mean_flow\",\n",
    "        \"daily_median_flow\",\n",
    "\n",
    "        # Long-term rainfall-runoff ratio benchmarks\n",
    "        \"rainfall_runoff_ratio_to_all\",\n",
    "        \"rainfall_runoff_ratio_to_annual\",\n",
    "        \"rainfall_runoff_ratio_to_monthly\",\n",
    "        \"rainfall_runoff_ratio_to_daily\",\n",
    "        \"rainfall_runoff_ratio_to_timestep\",\n",
    "\n",
    "         # Short-term rainfall-runoff ratio benchmarks\n",
    "        \"monthly_rainfall_runoff_ratio_to_monthly\",\n",
    "        \"monthly_rainfall_runoff_ratio_to_daily\",\n",
    "        \"monthly_rainfall_runoff_ratio_to_timestep\",\n",
    "\n",
    "        # Schaefli & Gupta (2007) benchmarks\n",
    "        \"scaled_precipitation_benchmark\",  # equivalent to \"rainfall_runoff_ratio_to_daily\"\n",
    "        \"adjusted_precipitation_benchmark\",\n",
    "        \"adjusted_smoothed_precipitation_benchmark\",\n",
    "     ]\n",
    "\n",
    "# Define periods for skill score calculation and corresponding masks\n",
    "periods = {\n",
    "    'calibration': 'cal_mask',\n",
    "    'validation': 'val_mask',\n",
    "    'all': None  # No mask for 'all'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5cbd0-0fd4-44d7-a06e-fcdd4b693fd2",
   "metadata": {},
   "source": [
    "### Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c59afb-289f-4c55-987c-4125715749f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# =============================\n",
    "# Process data\n",
    "\n",
    "# Read data\n",
    "pobs= pd.read_csv(pobs, index_col=0, sep='\\t') \n",
    "tobs= pd.read_csv(tobs, index_col=0, sep='\\t') \n",
    "qobs= pd.read_csv(qobs, index_col=0, sep='\\t') \n",
    "#cout= pd.read_csv(cout, index_col=0, sep='\\t', skiprows=1) \n",
    "cout= pd.read_csv(cout, index_col=0) \n",
    "geodata= pd.read_csv(geodata, index_col=0, sep='\\t') \n",
    "gauge_info= pd.read_csv(gauge_info, index_col=1)\n",
    "\n",
    "# convert index to datetime\n",
    "pobs.index = pd.to_datetime(pobs.index)\n",
    "tobs.index = pd.to_datetime(tobs.index)\n",
    "qobs.index = pd.to_datetime(qobs.index)\n",
    "cout.index = pd.to_datetime(cout.index)\n",
    "\n",
    "# Set index to int\n",
    "geodata.index = geodata.index.astype(int)\n",
    "gauge_info.index = gauge_info.index.astype(int)\n",
    "\n",
    "# Convert column headers to integers\n",
    "pobs.columns = pobs.columns.astype(int)\n",
    "tobs.columns = tobs.columns.astype(int)\n",
    "qobs.columns = qobs.columns.astype(int)\n",
    "cout.columns = cout.columns.astype(int)\n",
    "\n",
    "# trim to match start and end dates\n",
    "pobs = pobs.loc[start_date:end_date]\n",
    "tobs = tobs.loc[start_date:end_date]\n",
    "qobs = qobs.loc[start_date:end_date]\n",
    "cout = cout.loc[start_date:end_date]\n",
    "\n",
    "# Convert the calibration and validation ranges to Pandas Timestamps\n",
    "calibration_ranges = [(pd.Timestamp(start), pd.Timestamp(end)) for start, end in calibration_ranges]\n",
    "validation_ranges = [(pd.Timestamp(start), pd.Timestamp(end)) for start, end in validation_ranges]\n",
    "\n",
    "# replace missing values with nan in streamflow\n",
    "qobs.replace(-9999, np.nan, inplace=True)\n",
    "\n",
    "# =======================\n",
    "# Create upstream to downstream digraph\n",
    "riv_graph = nx.DiGraph()\n",
    "\n",
    "# Add edges from DataFrame\n",
    "for idx, row in geodata.iterrows():\n",
    "    if row['maindown'] != '0':  # Skip if maindown is '0'\n",
    "        riv_graph.add_edge(idx, row['maindown'])\n",
    "\n",
    "# =======================\n",
    "# Convert precipitation to m3\n",
    "        \n",
    "# Set area column to numeric\n",
    "geodata['area'] = pd.to_numeric(geodata['area'])\n",
    "\n",
    "# Create dictionary with subbasin ID and area\n",
    "area_dict = geodata['area'].to_dict()\n",
    "\n",
    "# Convert pobs from mm to m\n",
    "pobs= pobs / 1000 # mm to m\n",
    "\n",
    "# Multiply each column in pobs by the corresponding area value in area_dict to get m3\n",
    "for col in pobs.columns:\n",
    "    if col in area_dict:\n",
    "        pobs[col] *= area_dict[col]\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Test case\n",
    "gauge_info = gauge_info.loc[[58308]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "256791ef-cb36-477d-b5f0-99d4c4ef016a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    subbasin       KGE       NSE\n",
      "0      58223  0.610964  0.519060\n",
      "1      58213  0.803095  0.738158\n",
      "2      58208  0.864813  0.737735\n",
      "3      58408  0.342586 -0.488118\n",
      "4      58643  0.103904 -0.919058\n",
      "5      58308  0.109135 -0.056341\n",
      "6      58346  0.586765  0.500017\n",
      "7      58425  0.508368  0.233681\n",
      "8      58356 -0.337808  0.030087\n",
      "9      58363  0.117364  0.022042\n",
      "10     58418  0.157592  0.022224\n",
      "11     58290 -0.020381  0.153879\n",
      "12     58328  0.613651  0.521725\n",
      "13     58398 -0.312700  0.003075\n"
     ]
    }
   ],
   "source": [
    "# # # Define calibration periods\n",
    "# # calibration_ranges = [\n",
    "# #     ('1981-10-01', '2015-09-30')\n",
    "# # ]\n",
    "\n",
    "# # Functions for KGE and NSE\n",
    "# def compute_kge(simulated, observed):\n",
    "#     simulated = np.asarray(simulated, dtype=float)\n",
    "#     observed = np.asarray(observed, dtype=float)\n",
    "#     mask = ~np.isnan(simulated) & ~np.isnan(observed)\n",
    "#     simulated, observed = simulated[mask], observed[mask]\n",
    "#     if len(simulated) == 0 or len(observed) == 0:\n",
    "#         return np.nan\n",
    "#     mean_obs, mean_sim = np.mean(observed), np.mean(simulated)\n",
    "#     std_obs, std_sim = np.std(observed), np.std(simulated)\n",
    "#     if mean_obs == 0 or std_obs == 0 or std_sim == 0:\n",
    "#         return np.nan\n",
    "#     r = np.corrcoef(observed, simulated)[0, 1]\n",
    "#     beta, gamma = mean_sim / mean_obs, std_sim / std_obs\n",
    "#     if np.isnan(r) or np.isnan(beta) or np.isnan(gamma):\n",
    "#         return np.nan\n",
    "#     return 1 - np.sqrt((r - 1)**2 + (gamma - 1)**2 + (beta - 1)**2)\n",
    "\n",
    "# def compute_nse(simulated, observed):\n",
    "#     simulated = np.asarray(simulated, dtype=float)\n",
    "#     observed = np.asarray(observed, dtype=float)\n",
    "#     mask = ~np.isnan(simulated) & ~np.isnan(observed)\n",
    "#     simulated, observed = simulated[mask], observed[mask]\n",
    "#     if len(simulated) == 0 or len(observed) == 0:\n",
    "#         return np.nan\n",
    "#     return 1 - np.sum((observed - simulated)**2) / np.sum((observed - np.mean(observed))**2)\n",
    "\n",
    "# # Loop through all columns\n",
    "# results = []\n",
    "# for col in qobs.columns:\n",
    "#     qobs_col = qobs[col]\n",
    "#     cout_col = cout[col]\n",
    "    \n",
    "#     df_pair = pd.DataFrame({'qobs': qobs_col, 'cout': cout_col})\n",
    "    \n",
    "#     # Keep only rows within any calibration period\n",
    "#     mask = pd.Series(False, index=df_pair.index)\n",
    "#     for start, end in calibration_ranges:\n",
    "#         mask |= (df_pair.index >= pd.to_datetime(start)) & (df_pair.index <= pd.to_datetime(end))\n",
    "    \n",
    "#     df_calib = df_pair[mask].dropna()\n",
    "    \n",
    "#     kge = compute_kge(df_calib['cout'], df_calib['qobs'])\n",
    "#     nse = compute_nse(df_calib['cout'], df_calib['qobs'])\n",
    "    \n",
    "#     results.append({'subbasin': col, 'KGE': kge, 'NSE': nse})\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# results_df = pd.DataFrame(results)\n",
    "# print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0d22fb-5e29-4aa2-827c-b83817896014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define KGE function\n",
    "def compute_kge(simulated, observed):\n",
    "    \"\"\"\n",
    "    Computes KGE (Kling-Gupta Efficiency) between observed and simulated values.\n",
    "    \"\"\"\n",
    "    simulated = np.asarray(simulated, dtype=float)\n",
    "    observed = np.asarray(observed, dtype=float)\n",
    "\n",
    "    # Drop NaNs pairwise\n",
    "    mask = ~np.isnan(simulated) & ~np.isnan(observed)\n",
    "    simulated = simulated[mask]\n",
    "    observed = observed[mask]\n",
    "\n",
    "    if len(simulated) == 0 or len(observed) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    mean_obs = np.mean(observed)\n",
    "    mean_sim = np.mean(simulated)\n",
    "    std_obs = np.std(observed)\n",
    "    std_sim = np.std(simulated)\n",
    "\n",
    "    if mean_obs == 0 or std_obs == 0:  # avoid div by zero in ratios or corr\n",
    "        return np.nan\n",
    "\n",
    "    # Correlation (only if both series have variability)\n",
    "    if std_obs > 0 and std_sim > 0:\n",
    "        r = np.corrcoef(observed, simulated)[0, 1]\n",
    "    else:\n",
    "        r = np.nan\n",
    "\n",
    "    # print(r)\n",
    "\n",
    "    beta = mean_sim / mean_obs\n",
    "    gamma = std_sim / std_obs\n",
    "\n",
    "    if np.isnan(r) or np.isnan(beta) or np.isnan(gamma):\n",
    "        return np.nan\n",
    "\n",
    "    kge = 1 - np.sqrt((r - 1) ** 2 + (gamma - 1) ** 2 + (beta - 1) ** 2)\n",
    "    return kge\n",
    "\n",
    "    \n",
    "\n",
    "# Define funciton for calculating skill scores\n",
    "def calculate_skill_score(observed: pd.Series, simulated: pd.Series, benchmark: pd.Series, method: str ) -> float:\n",
    "    \"\"\"\n",
    "    Calculate skill score based on NSE (sum of squared errors) or RMSE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observed : pd.Series\n",
    "        Observed values.\n",
    "    simulated : pd.Series\n",
    "        Simulated values.\n",
    "    benchmark : pd.Series\n",
    "        Benchmark values to compare against.\n",
    "    method : str, optional\n",
    "        Skill score method: 'nse' (default) or 'rmse'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Skill score. Returns np.nan if benchmark error is zero.\n",
    "    \"\"\"\n",
    "    if method.lower() == 'nse':\n",
    "        # NSE-based: sum of squared errors\n",
    "        se_sim = ((observed - simulated) ** 2).sum()\n",
    "        se_bm = ((observed - benchmark) ** 2).sum()\n",
    "        skill_score = 1 - se_sim / se_bm if se_bm != 0 else np.nan\n",
    "\n",
    "    elif method.lower() == 'rmse':\n",
    "        # RMSE-based: root mean squared error\n",
    "        rmse_sim = np.sqrt(((observed - simulated) ** 2).mean())\n",
    "        rmse_bm = np.sqrt(((observed - benchmark) ** 2).mean())\n",
    "        skill_score = 1 - rmse_sim / rmse_bm if rmse_bm != 0 else np.nan\n",
    "\n",
    "    elif method.lower() == 'kge':\n",
    "        # KGE-based\n",
    "        kge_sim = compute_kge(simulated, observed)\n",
    "        kge_bm = compute_kge(benchmark, observed)\n",
    "\n",
    "        # print(kge_sim)\n",
    "        # print(kge_bm)\n",
    "    \n",
    "        # Return NaN if either KGE is undefined or benchmark is exactly 1\n",
    "        if np.isnan(kge_sim) or np.isnan(kge_bm) or kge_bm == 1:\n",
    "            return np.nan\n",
    "\n",
    "        skill_score = (kge_sim - kge_bm) / (1 - kge_bm)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'nse', 'rmse' or 'kge'.\")\n",
    "\n",
    "    return skill_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc2ec8-7bfd-4824-91d6-8c3e64677767",
   "metadata": {},
   "source": [
    "### Calculate Skill Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e951369e-40dc-458b-bb3d-aee3ea48bb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Subbasin 58308\n",
      "Calculating Benchmarks\n",
      "WARNING: the annual_mean_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "WARNING: the annual_median_flow benchmark cannot be used to predict unseen data. See docstring for details.\n",
      "Calculating Skill Score for: calibration\n",
      "Calculating Skill Score for: validation\n",
      "Calculating Skill Score for: all\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# Global dictionary to store skill scores for all subbasins\n",
    "all_subbasin_scores = {}\n",
    "\n",
    "# Iterate over each subbasin in list\n",
    "for subbasin in gauge_info.index:\n",
    "\n",
    "    print(f'Analyzing Subbasin {subbasin}')\n",
    "\n",
    "    # =================================\n",
    "    # Create HydroBM input\n",
    "    \n",
    "    # Find upstream segments for the given subbasin\n",
    "    upstream_segments = list(nx.ancestors(riv_graph, subbasin))\n",
    "    \n",
    "    # Add the target segment to the upstream segments\n",
    "    upstream_segments.append(subbasin)\n",
    "\n",
    "\n",
    "    # Sum upstream precipitation\n",
    "    precipitation_sum = pd.DataFrame(\n",
    "        pobs[upstream_segments].sum(axis=1),\n",
    "        columns=['precipitation']\n",
    "    )\n",
    "    \n",
    "    # Mean upstream temperature\n",
    "    temperature_mean = pd.DataFrame(\n",
    "        tobs[upstream_segments].mean(axis=1),\n",
    "        columns=['temperature']\n",
    "    )\n",
    "\n",
    "    # Create hydrobm input dataframe\n",
    "    bm_input = pd.DataFrame({\n",
    "        'streamflow': qobs[subbasin] * 84600,  # Streamflow volume for the given subbasin\n",
    "        'precipitation': precipitation_sum['precipitation'],  # Sum of upstream precipitation in m3\n",
    "        'temperature': temperature_mean['temperature'] # mean upstream temperature of the subbasin\n",
    "    })\n",
    "\n",
    "    # Create the cal_mask column\n",
    "    bm_input['cal_mask'] = bm_input.index.to_series().apply(\n",
    "        lambda x: any(pd.to_datetime(start) <= x <= pd.to_datetime(end) for start, end in calibration_ranges)\n",
    "    )\n",
    "\n",
    "    # Create the val_mask column\n",
    "    bm_input['val_mask'] = bm_input.index.to_series().apply(\n",
    "        lambda x: any(pd.to_datetime(start) <= x <= pd.to_datetime(end) for start, end in validation_ranges)\n",
    "    )\n",
    "\n",
    "    print('Calculating Benchmarks')\n",
    "    \n",
    "    # Calculate the benchmarks and scores\n",
    "    benchmark_flows, scores = calc_bm(\n",
    "        bm_input,\n",
    "\n",
    "        # Time period selection\n",
    "        bm_input['cal_mask'],\n",
    "        val_mask=bm_input['val_mask'],\n",
    "\n",
    "        # Variable names in 'data'\n",
    "        precipitation=\"precipitation\",\n",
    "        streamflow=\"streamflow\",\n",
    "\n",
    "        # Benchmark choices\n",
    "        benchmarks=benchmarks,\n",
    "        metrics=['nse', 'kge'],\n",
    "        optimization_method=\"brute_force\",\n",
    "\n",
    "        # Snow model inputs\n",
    "        calc_snowmelt=True,\n",
    "        temperature=\"temperature\",\n",
    "        snowmelt_threshold=0.0,\n",
    "        snowmelt_rate=3.0,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # ====================================\n",
    "    # Prepare to calculate skill scores\n",
    "    \n",
    "    # Prepare observed and simulated flows as DataFrames\n",
    "    obs_df = pd.DataFrame({'observed_flow': qobs[subbasin] * 86400})\n",
    "    sim_df = pd.DataFrame({'simulated_flow': cout[subbasin] * 86400})\n",
    "\n",
    "    # Prepare cal and val masks as DataFrames\n",
    "    cal_mask_df = pd.DataFrame({'cal_mask': bm_input['cal_mask']})\n",
    "    val_mask_df = pd.DataFrame({'val_mask': bm_input['val_mask']})\n",
    "    \n",
    "    # Merge onto benchmark_flows using index\n",
    "    benchmark_flows = benchmark_flows.merge(obs_df, left_index=True, right_index=True, how='left')\n",
    "    benchmark_flows = benchmark_flows.merge(sim_df, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    # Merge masks onto benchmark_flows using index\n",
    "    benchmark_flows = benchmark_flows.merge(cal_mask_df, left_index=True, right_index=True, how='left')\n",
    "    benchmark_flows = benchmark_flows.merge(val_mask_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # ======================\n",
    "    # Calculate skill scores\n",
    "\n",
    "    # Get list of benchmark columns\n",
    "    bm_columns = [col for col in benchmark_flows.columns if col.startswith('bm_')]\n",
    "\n",
    "    # Dictionary to store results\n",
    "    skill_scores = {period: {} for period in periods}\n",
    "\n",
    "    # Iterate over periods\n",
    "    for period_name, mask_col in periods.items():\n",
    "\n",
    "        print(f'Calculating Skill Score for: {period_name}')\n",
    "\n",
    "        # Trim to only required period\n",
    "        if mask_col is not None:\n",
    "            df_period = benchmark_flows[benchmark_flows[mask_col]]\n",
    "        else:\n",
    "            df_period = benchmark_flows.copy()\n",
    "            \n",
    "        \n",
    "        for bm_col in bm_columns:\n",
    "\n",
    "            # # Debug: print std deviation of benchmark column\n",
    "            # std_bm = df_period[bm_col].std()\n",
    "            # print(f\"Period={period_name}, Benchmark={bm_col}, Std={std_bm:.6f}\")\n",
    "\n",
    "            # Calculate skill score based on metric\n",
    "            skill_score = calculate_skill_score(\n",
    "                observed=df_period['observed_flow'],\n",
    "                simulated=df_period['simulated_flow'],\n",
    "                benchmark=df_period[bm_col],\n",
    "                method=skill_score_metric  # nse or rmse \n",
    "            )\n",
    "\n",
    "            # Remove benchmarks that don't work on unseen data\n",
    "            if bm_col in ['bm_annual_mean_flow', 'bm_annual_median_flow'] and period_name in ['calibration', 'validation', 'all']:\n",
    "                skill_score = np.nan\n",
    "        \n",
    "            # Store\n",
    "            skill_scores[period_name][bm_col] = skill_score\n",
    "\n",
    "    # Store subbasin results in the global dictionary\n",
    "    all_subbasin_scores[subbasin] = skill_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e7b5e9-1320-439c-9643-1f09a523ec91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skill scores saved to ./skill_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Save outputs\n",
    "\n",
    "# Convert global dictionary to a multi-index DataFrame for easy access\n",
    "skill_scores_df = pd.concat({\n",
    "    subbasin: pd.DataFrame(sub_scores) \n",
    "    for subbasin, sub_scores in all_subbasin_scores.items()\n",
    "}, names=['subbasin', 'benchmark'])\n",
    "\n",
    "# Reset MultiIndex to get subbasin and benchmark as columns\n",
    "skill_scores_long = skill_scores_df.reset_index()\n",
    "skill_scores_long = skill_scores_long.rename(columns={'level_0': 'subbasin', 'level_1': 'benchmark'})\n",
    "\n",
    "# Now, melt the periods into a single column\n",
    "skill_scores_long = skill_scores_long.melt(\n",
    "    id_vars=['subbasin', 'benchmark'],\n",
    "    value_vars=['calibration', 'validation', 'all'],\n",
    "    var_name='period',\n",
    "    value_name='skill_score'\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "output_file = os.path.join(output_dir, 'skill_scores.csv')\n",
    "skill_scores_long.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Skill scores saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9dd6b7-0fcd-47b6-827e-01e1a8c3b90e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scienv",
   "language": "python",
   "name": "scienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
